
--- Page 1 ---
Basics of Machine Learning
Dariush Salami
3rd Feb 2026
Introduction to AI, ML, and LLMs 1
--- Page 2 ---
Course schedule
01 Introduction to AI, ML, and LLMs
02 Basics of Machine Learning  (Industrial talk)
03 Data Preprocessing for NLP
04 Introduction to LLMs and Their Applications
05 Supervised Learning for NLP Tasks
06 Advanced NLP Techniques
07 Language Generation with LLMs
08 Ethical Considerations and Challenges in LLMs
09 Model Deployment and Integration in Applications
10 Final Project and Presentation
Introduction to AI, ML, and LLMs 2
--- Page 3 ---
Let us ask the same question now
Introduction to AI, ML, and LLMs 3
What is this object?
Itâ€™s a car!
--- Page 4 ---
What is Machine Learning?
Introduction to AI, ML, and LLMs 4
Dataset
+ +
 +
Extract required patterns

--- Page 5 ---
Neural language models
Introduction to AI, ML, and LLMs 5
à·‘
ğ‘–=1
ğ‘
ğ‘ƒ ğ‘¥ ğ‘¡+1 |ğ‘¥ ğ‘¡ , â€¦ , ğ‘¥ ğ‘–âˆ’1
I have a dog whose name is 
Lucy. I have two ğ‘“(ğœƒ)
A differentiable 
function (e.g. a 
neural 
network)
A distribution over
the vocabulary

--- Page 6 ---
Basics of Machine 
Learning
Introduction to AI, ML, and LLMs
6
--- Page 7 ---
Three types 
of machine 
learning
â€¢ Supervised learning
â€¢ Given data ğ· = {(ğ’™ğ’Š, ğ‘¦ğ‘–)}ğ‘–=1
ğ‘ , learn to 
predict the output ğ‘¦ğ‘– using input ğ’™ğ’Š.
â€¢ elements of ğ’™ğ’Š are called features, 
attributes, or covariates.
â€¢ ğ‘¦ğ‘– is called the response or label.
â€¢ Unsupervised learning
â€¢ Given only inputs ğ· = {ğ’™ğ’Š}ğ‘–=1
ğ‘  
(unlabeled data) the goal is to find 
some â€œinterestingâ€ patterns in the data.
â€¢ Reinforcement learning
â€¢ Broadly: training an agent to take 
actions that maximize some reward in 
the long term. For example: training a 
robot to escape a maze.
https://uk.mathworks.com/discovery/reinforcement-learning.html
Introduction to AI, ML, and LLMs 7
--- Page 8 ---
Supervised learning
Introduction to AI, ML, and LLMs 8
--- Page 9 ---
Unsupervised learning: Clustering
Introduction to AI, ML, and LLMs 9

--- Page 10 ---
Unsupervised learning: Dimensionality reduction
Introduction to AI, ML, and LLMs 10
Explain high-dimensional data in a lower-dimensional subspace which 
captures the "essential" parts of the original data.

--- Page 11 ---
Data matrix (1/2)
â€¢ Usually (but not always!) rows are individual data points; columns 
are characteristics of the data points (e.g. measurement, feature, 
attribute, variable, covariate).
Introduction to AI, ML, and LLMs 11

--- Page 12 ---
Data matrix (2/2)
Introduction to AI, ML, and LLMs 12

--- Page 13 ---
Feature vector
â€¢ One row of the data matrix, written as a column vector, represents one 
data point ğ’™ğ’Š.
â€¢ ğ’™ğ’Š is called e.g. a feature vector or a representation.
â€¢ Everything can not be represented as vectors, but a lot of things can.
â€¢ To emphasize that ğ’™ğ’Š is a vector, it is often written in bold.
â€¢ We assume the following operations to be familiar: vector addition ğ’™ +
ğ’š, vector inner product ğ’™ğ‘»ğ’š, vector length (norm) ğ’™ , matrix 
multiplication ğ´ğ’™, matrix addition ğ´ + ğµ, matrix product ğ´ğµ, matrix 
eigenvalues and eigenvectors ğ´ğ’™ = ğœ†ğ’™.
Introduction to AI, ML, and LLMs 13

--- Page 14 ---
Histogram (1/2)
â€¢ Histogram is a common way to create features and explore data, 
by dividing values of a variable in different bins.
â€¢ Height of a box in a histogram is equal to the number of values in 
the corresponding bin.
Introduction to AI, ML, and LLMs 14

--- Page 15 ---
Histogram (2/2)
â€¢ A color histogram of an image shows the number of pixels in each 
color.
â€¢ Histograms can be used also for text and audio.
Introduction to AI, ML, and LLMs 15

--- Page 16 ---
Text Features
Introduction to AI, ML, and LLMs
16
--- Page 17 ---
Text as a vector
â€¢ A term-document matrix:
â€¢ Rows correspond to text documents, columns corresponds to 
words, and ğ‘¥ğ‘–ğ‘— is the number of occurrences of word ğ‘— in 
document ğ‘–.
â€¢ Bag of words feature vector: ğ‘¥ğ‘–ğ‘— = 1 iff document ğ‘– contains word 
ğ‘—.
Introduction to AI, ML, and LLMs 17

--- Page 18 ---
Example: student assignments
â€¢ Example: A term-document matrix. Word 
frequency â‰¥0 is represented with a black 
dot.
â€¢ Left-ordered form: columns are ordered 
according to appearance of words in 
documents: the first document has more 
than 200 words, the second document 
has also roughly 200 words of which 150 
did not appear in the first, etc.
â€¢ Note: the data include a document which 
is almost identical to another document. 
Which one?
Introduction to AI, ML, and LLMs 18
--- Page 19 ---
tf-idf text representation (term frequency â€“ inverse document frequency)
â€¢ The most common words (and, so, ...) can be 
removed manually.
â€¢ Words are often weighted according to their 
importance using the inverse document 
frequency.
â€¢ where w(word) is the weight, df(word) is the 
document frequency (how many documents have 
the word) and N is the number of documents.
â€¢ Word frequencies are multiplied by the 
corresponding weights w.
â€¢ For example, the word â€andâ€ which likely appears 
in every document gets weight 0.
Introduction to AI, ML, and LLMs 19
--- Page 20 ---
Neural network -based text 
representations
â€¢ Nowadays, several large neural network 
models, pre-trained with massive data 
sets, have been published for Natural 
Language Processing (NLP).
â€¢ or example, BERT learns a vector 
representation for a word, sentence, or 
the whole document.
Introduction to AI, ML, and LLMs 20
https://jalammar.github.io/illustrated-bert/
--- Page 21 ---
Example: Countries and capitals
â€¢ 2D PCA projection of 1000D word vectors 
learned using skip-gram neural network 
model.
â€¢ For example: Madrid - Spain + France = 
Paris
Introduction to AI, ML, and LLMs 21
--- Page 22 ---
Image Features
Introduction to AI, ML, and LLMs
22
--- Page 23 ---
Convolution (1/2)
â€¢ Convolution is a generic operation which forms new features as 
the weighted sum of neighboring pixels.
â€¢ Weights are defined by a kernel (or filter)
â€¢ The output (or feature map) is obtained by multiplying each image patch 
elementwise with the kernel.
â€¢ A typical kernel size with image data is 3*3.
Introduction to AI, ML, and LLMs 23
https://anhreynolds.com/blogs/cnn.html
--- Page 24 ---
Convolution (2/2)
â€¢ In practice the 
elements of the 
kernel (weights, 
parameters) are not 
set by hand but 
learned from data to 
result in useful 
features.
Introduction to AI, ML, and LLMs 24
https://en.m.wikipedia.org/wiki/File:2D_Convolution_Animation.gif

--- Page 25 ---
Convolution, example
â€¢ A model trained to classify images of digits 0,1,...,9.
â€¢ Learned filters and a feature map (after one convolutional
â€¢ layer) for an input image of digit 9.
Introduction to AI, ML, and LLMs 25
https://medium.com/dataseries/visualizing-the-feature-maps-and-
filtersby-convolutional-neural-networks-e1462340518e

--- Page 26 ---
Convolution for smoothing
â€¢ Convolution can be used to average across neighboring pixels,
â€¢ resulting in a smoothed image.
Introduction to AI, ML, and LLMs 26
https://stackoverflow.com/questions/34851259/how-to-make-a-
smoothkernel-in-convolution-neural-networks-with-mxnet-framework

--- Page 27 ---
Regression and 
Classification
Introduction to AI, ML, and LLMs
27
--- Page 28 ---
Linear Regression
Introduction to AI, ML, and LLMs 28

--- Page 29 ---
Linear Regression
â€¢ What do we try to find with linear 
regression?
â€¢ How do we find proper parameters w0 and 
w1 ?
Introduction to AI, ML, and LLMs 29
--- Page 30 ---
Linear Regression
Introduction to AI, ML, and LLMs 30

--- Page 31 ---
Linear Regression
â€¢ Loss function: estimates quality of current 
solution
â€¢ It is sometimes called error function or cost 
function.
Introduction to AI, ML, and LLMs 31

--- Page 32 ---
Linear Regression
Introduction to AI, ML, and LLMs 32

--- Page 33 ---
Logistic regression
Introduction to AI, ML, and LLMs 33
â€¢ Classes might be nominal in real-world problems.

--- Page 34 ---
Logistic regression
â€¢ Classes might be nominal in real-
world problems.
â€¢ Weather: sunny, rainy
â€¢ Medical: positive diagnosis, 
negative diagnosis
â€¢ Localization: indoor, outdoor
â€¢ In such case, classification is binary: 
y âˆˆ {0, 1}
â€¢ Linear regression: h(x) can be 
smaller than 0 or greater than 1
â€¢ Logistic regression: 0 â‰¤ h(x) â‰¤ 1
Introduction to AI, ML, and LLMs 34
--- Page 35 ---
Logistic regression
Introduction to AI, ML, and LLMs 35

--- Page 36 ---
Logistic regression
Introduction to AI, ML, and LLMs 36

--- Page 37 ---
Multiclass classification
â€¢ Multi-class: One-versus all:
â€¢ Train classifiers for each class to 
obtain probability that x belongs to 
class i:
â€¢ then, choose
Introduction to AI, ML, and LLMs 37

--- Page 38 ---
Clustering
Introduction to AI, ML, and LLMs
38
--- Page 39 ---
Summary supervised classification 
algorithms
Introduction to AI, ML, and LLMs 39

--- Page 40 ---
Unsupervised learning
Introduction to AI, ML, and LLMs 40
â€¢ Supervised: {(x1,1, x1,2) â†’ y1, (x2,1, x2,2) â†’ y2, . . . , (xn,1, xn,2) â†’ yn}
â€¢ Unsupervised: {(x1,1, x1,2), (x2,1, x2,2), . . . , (xn,1, xn,2)}

--- Page 41 ---
K-means Algorithm
â€¢ Iteratively find k clusters in the data
â€¢ Init: Randomly choose k points as initial 
cluster centroids
â€¢ Repeat:
â€¢ Assign data points xi , i âˆˆ {1..n} to 
these cluster centroids conditioned 
on distance: Cj = {xi |cj is nearest 
centroid to xi}
â€¢ Move cluster centroids to the center 
weight of the points associated to 
them
Introduction to AI, ML, and LLMs 41
--- Page 42 ---
Introduction to AI, ML, and LLMs 42

--- Page 43 ---
Model validation and 
Selection
Introduction to AI, ML, and LLMs
43
--- Page 44 ---
Data, data, data, ... 
or not (?)
â€¢ Use separate data sets
â€¢ Feature selection: Identify meaningful features
â€¢ Training: Train a model with given features
â€¢ Testing: Test a trained model and features
â€¢ Model selection: Find a best model given 
features
â€¢ Using the same set for multiple purposes 
may result in biased results
Introduction to AI, ML, and LLMs 44
--- Page 45 ---
Preparation for 
training and testing
â€¢ Data scarcity: Most data for training, rest for 
testing
â€¢ Diversity: Use several runs with different data 
sets
â€¢ Randomness: Avoid deterministic separation 
of data
â€¢ Correlation: Training and testing data from 
different sessions where possible
â€¢ Domination: Class-sizes during training should 
be equal
Introduction to AI, ML, and LLMs 45

--- Page 46 ---
Training the model â€“ varying data distributions
Introduction to AI, ML, and LLMs 46
â€¢ Builds: Multiple distributions in training and testing data
â€¢ Avoid: random generation of training/testing sets from same data 
â†’ correlation

--- Page 47 ---
Training the model on scarce data: 
Leave-one-out cross-validation
â€¢ n-fold cross-validation where n is the 
number of sample instances
â€¢ Leave out each instance once; train 
model on remaining instances
â€¢ Estimate performance on left-out 
instances (success/failure)
â€¢ Caution: Possible correlation from 
data sampled in same condition
Introduction to AI, ML, and LLMs 47
--- Page 48 ---
Bias â€“ Variance Tradeoff
Introduction to AI, ML, and LLMs
48
--- Page 49 ---
Bias - Variance tradeoff
Introduction to AI, ML, and LLMs 49

--- Page 50 ---
Bias - Variance tradeoff
Introduction to AI, ML, and LLMs 50
â€¢ Sample points are created for the function sin(2Ï€x) + N where N is 
a random noise value.

--- Page 51 ---
Bias - Variance tradeoff
Introduction to AI, ML, and LLMs 51
â€¢ We fit the data points into a polynomial function:

--- Page 52 ---
Bias - Variance tradeoff
Introduction to AI, ML, and LLMs 52
â€¢ We fit the data points into a polynomial function:
â€¢ This can be obtained by minimizing a loss function which 
measures the misfit between â„ ğ‘¥, ğ‘¤  and the training data set:

--- Page 53 ---
Bias - Variance tradeoff
Introduction to AI, ML, and LLMs 53
â€¢ One problem is the right choice of the dimension M
â€¢ When M is too small, the approximation accuracy might be bad

--- Page 54 ---
Bias - Variance tradeoff
Introduction to AI, ML, and LLMs 54
â€¢ Visualize loss L[(X , Y), h(Â·)] w.r.t. the data by Root of the Mean 
Squared (RMS)

--- Page 55 ---
Bias - Variance tradeoff
Introduction to AI, ML, and LLMs 55
â€¢ Visualize loss L[(X , Y), h(Â·)] w.r.t. the data by Root of the Mean 
Squared (RMS)

--- Page 56 ---
Bias - Variance tradeoff
Introduction to AI, ML, and LLMs 56
â€¢ Visualize loss L[(X , Y), h(Â·)] w.r.t. the data by Root of the Mean 
Squared (RMS)

--- Page 57 ---
Bias - Variance tradeoff
Introduction to AI, ML, and LLMs 57
â€¢ Visualize loss L[(X , Y), h(Â·)] w.r.t. the data by Root of the Mean 
Squared (RMS)

--- Page 58 ---
Bias - Variance tradeoff
Introduction to AI, ML, and LLMs 58
â€¢ Visualize loss L[(X , Y), h(Â·)] w.r.t. the data by Root of the Mean 
Squared (RMS)

--- Page 59 ---
Bias - Variance 
tradeoff
â€¢ This event is called overfitting
â€¢ The polynomial is now trained too well to the 
training data
â€¢ It performs badly on test data
Introduction to AI, ML, and LLMs 59
--- Page 60 ---
Bias - Variance tradeoff
Introduction to AI, ML, and LLMs 60
â€¢ With increasing number of data points, the problem of overfitting
â€¢ becomes less severe for a given value of M.

--- Page 61 ---
Bias and Variance in training a learning model
Introduction to AI, ML, and LLMs 61
â€¢ Bias
â€¢ Inability of machine learning model to capture the true distribution of the 
data
â€¢ Example: Linear model to describe non-linear relationship between data and labels
â€¢ e.g. linear regression is expected to have a high bias error; in contrast to other algorithms that take less 
hard assumptions (e.g. decision trees, k-Nearest Neighbors, Support Vector Machines)
â€¢ High bias: more assumption in the learning algorithm on the underlying 
distribution
â€¢ Low bias: fewer assumptions in the learning algorithm
--- Page 62 ---
Bias and Variance in training a learning model
Introduction to AI, ML, and LLMs 62
â€¢ Variance
â€¢ Model overfits on a particular dataset (learning to fit very closely to the 
points of a particular dataset)
â€¢ Example: Generally, nonlinear machine learning algorithms like decision trees have 
a high variance
â€¢ Low variance algorithms: Linear regression, logistic regression, linear 
discriminant analysis
â€¢ High variance algorithms: Decision Trees, k-NN, support vector machines
--- Page 63 ---
Model selection
Introduction to AI, ML, and LLMs 63

--- Page 64 ---
Learning curves
Introduction to AI, ML, and LLMs 64
â€¢ Plotting learning curves helps to find out, whether our algorithm 
suffers from high variance or high bias.

--- Page 65 ---
Learning curves
Introduction to AI, ML, and LLMs 65

--- Page 66 ---
Learning curves
Introduction to AI, ML, and LLMs 66

--- Page 67 ---
Evaluation of model 
performance
Introduction to AI, ML, and LLMs
67
--- Page 68 ---
Evaluation of model performance
Introduction to AI, ML, and LLMs 68
â€¢ Classification accuracy
â€¢ Confusion matrices
â€¢ Precision
â€¢ Recall
â€¢ F1-score

--- Page 69 ---
Evaluation of model performance
Introduction to AI, ML, and LLMs 69
Write down the formula!
--- Page 70 ---
Evaluation of model performance
Introduction to AI, ML, and LLMs 70

--- Page 71 ---
Evaluation of model performance
Introduction to AI, ML, and LLMs 71

--- Page 72 ---
Evaluation of model performance
Introduction to AI, ML, and LLMs 72
â€¢ Tradeoff between precision and recall.
â€¢ Predict a particular class only if very confident â‡’ High precision 
(minimize false positives)
â€¢ Minimize false negatives â‡’ High recall
â€¢ F1-score combines precision and recall into a single decision 
variable:

--- Page 73 ---
Questions?
dariush.salami@aalto.fi
Introduction to AI, ML, and LLMs
73