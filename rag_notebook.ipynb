{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Application for Domain-Specific Question Answering\n",
    "\n",
    "This notebook demonstrates a Retrieval-Augmented Generation (RAG) system for answering questions based on custom documents.\n",
    "\n",
    "## Features\n",
    "- Load and process documents (PDF, TXT, DOCX)\n",
    "- Create vector embeddings using OpenAI\n",
    "- Store embeddings in ChromaDB\n",
    "- Answer questions using retrieved context\n",
    "- Show source documents for transparency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-community langchain-openai chromadb pypdf python-docx python-dotenv openai tiktoken -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set up your OpenAI API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Set your OpenAI API key\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "# Configuration\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "LLM_MODEL = \"gpt-3.5-turbo\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "TOP_K_RESULTS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader, Docx2txtLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Process Documents\n",
    "\n",
    "You can upload your own documents or create sample documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample documents if needed\n",
    "import os\n",
    "\n",
    "# Create documents directory\n",
    "os.makedirs('sample_docs', exist_ok=True)\n",
    "\n",
    "# Sample document 1: Machine Learning\n",
    "ml_content = \"\"\"Machine Learning Best Practices\n",
    "\n",
    "Machine learning is a powerful tool for solving complex problems. Here are key best practices:\n",
    "\n",
    "1. Data Quality: Always ensure your training data is clean and representative.\n",
    "2. Model Selection: Start with simple models before moving to complex ones.\n",
    "3. Validation: Use cross-validation to get robust performance estimates.\n",
    "4. Overfitting: Apply regularization techniques to prevent overfitting.\n",
    "5. Evaluation: Choose appropriate metrics for your specific problem.\n",
    "\"\"\"\n",
    "\n",
    "# Sample document 2: NLP\n",
    "nlp_content = \"\"\"Natural Language Processing Guide\n",
    "\n",
    "NLP focuses on enabling computers to understand human language. Key concepts:\n",
    "\n",
    "1. Tokenization: Breaking text into words or sentences.\n",
    "2. Embeddings: Converting words into numerical vectors.\n",
    "3. Transformers: Modern architecture that revolutionized NLP.\n",
    "4. BERT and GPT: Popular pre-trained models for various NLP tasks.\n",
    "5. Fine-tuning: Adapting pre-trained models to specific tasks.\n",
    "\"\"\"\n",
    "\n",
    "# Sample document 3: RAG Systems\n",
    "rag_content = \"\"\"Retrieval-Augmented Generation Systems\n",
    "\n",
    "RAG combines retrieval with generation for better AI responses:\n",
    "\n",
    "1. Document Indexing: Store documents in a vector database.\n",
    "2. Retrieval: Find relevant documents based on query similarity.\n",
    "3. Generation: Use LLM to generate answers from retrieved context.\n",
    "4. Benefits: Reduced hallucinations, up-to-date information, source attribution.\n",
    "5. Use Cases: Customer support, technical documentation, research assistance.\n",
    "\"\"\"\n",
    "\n",
    "# Write sample documents\n",
    "with open('sample_docs/ml_best_practices.txt', 'w') as f:\n",
    "    f.write(ml_content)\n",
    "\n",
    "with open('sample_docs/nlp_guide.txt', 'w') as f:\n",
    "    f.write(nlp_content)\n",
    "\n",
    "with open('sample_docs/rag_systems.txt', 'w') as f:\n",
    "    f.write(rag_content)\n",
    "\n",
    "print(\"‚úÖ Sample documents created in 'sample_docs' directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents from directory\n",
    "def load_documents_from_directory(directory: str) -> List:\n",
    "    \"\"\"Load all text documents from a directory\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        if filename.endswith('.txt'):\n",
    "            loader = TextLoader(file_path)\n",
    "            documents.extend(loader.load())\n",
    "            print(f\"Loaded: {filename}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Load documents\n",
    "docs = load_documents_from_directory('sample_docs')\n",
    "print(f\"\\n‚úÖ Loaded {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Split Documents into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "print(f\"‚úÖ Split into {len(chunks)} chunks\")\n",
    "\n",
    "# Show a sample chunk\n",
    "if chunks:\n",
    "    print(f\"\\nSample chunk:\\n{chunks[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Vector Store\n",
    "\n",
    "Create embeddings and store them in ChromaDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)\n",
    "\n",
    "# Create vector store\n",
    "print(\"Creating vector store... (this may take a moment)\")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Vector store created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Retrieval\n",
    "\n",
    "Test if we can retrieve relevant documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test similarity search\n",
    "query = \"What is RAG?\"\n",
    "relevant_docs = vectorstore.similarity_search(query, k=2)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Found {len(relevant_docs)} relevant documents:\\n\")\n",
    "\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"Document {i}:\")\n",
    "    print(doc.page_content[:300])\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create RAG Chain\n",
    "\n",
    "Set up the Retrieval-Augmented Generation chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=LLM_MODEL, temperature=0.0)\n",
    "\n",
    "# Create custom prompt template\n",
    "prompt_template = \"\"\"You are a helpful assistant that answers questions based on the provided context.\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer based on the context, say so - don't make up information.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": TOP_K_RESULTS}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG chain created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ask Questions\n",
    "\n",
    "Now you can ask questions based on your documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question: str):\n",
    "    \"\"\"Ask a question and display the answer with sources\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"‚ùì Question: {question}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    response = qa_chain.invoke({\"query\": question})\n",
    "    \n",
    "    print(f\"üí° Answer:\\n{response['result']}\\n\")\n",
    "    \n",
    "    if response.get('source_documents'):\n",
    "        print(f\"\\nüìÑ Sources ({len(response['source_documents'])} documents):\")\n",
    "        for i, doc in enumerate(response['source_documents'], 1):\n",
    "            source = doc.metadata.get('source', 'Unknown')\n",
    "            print(f\"\\n[{i}] {source}\")\n",
    "            print(f\"    {doc.page_content[:150]}...\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Questions\n",
    "\n",
    "Try these example questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: About RAG\n",
    "ask_question(\"What is Retrieval-Augmented Generation and what are its benefits?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: About Machine Learning\n",
    "ask_question(\"What are some best practices for machine learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: About NLP\n",
    "ask_question(\"What are transformers in NLP and why are they important?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Comparative question\n",
    "ask_question(\"How does RAG help reduce hallucinations in AI systems?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Interactive Question-Answering\n",
    "\n",
    "Run this cell for interactive Q&A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive mode\n",
    "print(\"\\nü§ñ Interactive RAG Q&A\")\n",
    "print(\"Type 'exit' to quit\\n\")\n",
    "\n",
    "while True:\n",
    "    question = input(\"\\n‚ùì Your question: \").strip()\n",
    "    \n",
    "    if question.lower() in ['exit', 'quit', 'q']:\n",
    "        print(\"üëã Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    if not question:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        ask_question(question)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "This notebook demonstrates a complete RAG system with:\n",
    "\n",
    "‚úÖ Document loading and processing  \n",
    "‚úÖ Vector embeddings with OpenAI  \n",
    "‚úÖ ChromaDB vector store  \n",
    "‚úÖ Semantic search and retrieval  \n",
    "‚úÖ LLM-based answer generation  \n",
    "‚úÖ Source attribution  \n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To use this with your own documents:\n",
    "1. Replace the sample documents with your own files\n",
    "2. Adjust chunk size and overlap based on your documents\n",
    "3. Experiment with different embedding models\n",
    "4. Try different LLM models (GPT-4, etc.)\n",
    "5. Tune the number of retrieved documents (TOP_K_RESULTS)\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [ChromaDB Documentation](https://docs.trychroma.com/)\n",
    "- [OpenAI API Documentation](https://platform.openai.com/docs/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
